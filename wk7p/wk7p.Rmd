---
title: "wk7p"
output: html_document
date: "2023-12-10"
---
research question:
Are the values (in this case the density of blue plaques) similar (or dissimilar) across the wards of London
# data download
```{r}
library(here)
library(janitor)
library(sf)
library(tidyverse)
#read the ward data in
LondonWards <- st_read(here::here("statistical-gis-boundaries-london", 
                                  "ESRI", "London_Ward.shp"))
```
```{r}
LondonWardsMerged <- st_read(here::here("statistical-gis-boundaries-london", 
                                        "ESRI",
                                        "London_Ward_CityMerged.shp"))%>%
  st_transform(.,27700)
```
```{r}
WardData <- read_csv("https://data.london.gov.uk/download/ward-profiles-and-atlas/772d2d64-e8c6-46cb-86f9-e52b4c7851bc/ward-profiles-excel-version.csv",
                    locale = locale(encoding = "latin1"),
                     na = c("NA", "n/a")) %>% 
  clean_names()

LondonWardsMerged <- LondonWardsMerged %>% 
  left_join(WardData, 
            by = c("GSS_CODE" = "new_code"))%>%
  dplyr::distinct(GSS_CODE, .keep_all = T)%>%
  dplyr::select(GSS_CODE, ward_name, average_gcse_capped_point_scores_2014)
```
It’s probably projected correctly, but in case it isn’t give it a projection 
using the st_crs() function in the sf package
```{r}
#have a look to check that it's in the right projection
st_crs(LondonWardsMerged)
```
Always plot the data to check it looks reasonable (e.g. there are no points in 
other countries / the latitude and longitude are correct)
```{r}
library(tmap)

BluePlaques <- st_read(here::here("open-plaques-london-2018-04-08.geojson")) %>%
    st_transform(.,27700)
```
```{r}
tmap_mode("plot")
tm_shape(LondonWardsMerged) +
  tm_polygons(col = NA, alpha = 0.5) +
tm_shape(BluePlaques) +
  tm_dots(col = "blue")
```
# Data cleaning
we might need to lose the blue plaques that fall outside of London
```{r}
summary(BluePlaques)
```
```{r}
BluePlaquesSub <- BluePlaques[LondonWardsMerged,]

tm_shape(LondonWardsMerged) +
  tm_polygons(col = NA, alpha = 0.5) +
tm_shape(BluePlaquesSub) +
  tm_dots(col = "blue")
```
# Data manipulation（数据处理）
The measures of spatial autocorrelation that we will be using require continuous observations (counts of blue plaques, average GCSE scores, average incomes etc.) to be spatially referenced (i.e. attached to a spatial unit like a ward or a borough). The file you have already has the various obervations associated with the London Ward data file already attached to it, but let’s continue with our blue plaques example for now.
我们将使用的空间自相关性度量需要连续观测数据（例如蓝色牌匾的数量、GCSE平均分数、平均收入等）与空间单元（如选区或自治市区）相关联。你已经拥有的文件中包含了与伦敦选区数据文件相关联的各种观测数据，但现在我们继续以蓝色牌匾为例进行讨论。

To create a continuous observation from the blue plaques data we need to count all of the blue plaques that fall within each Ward in the City. Luckily, we can do this using the st_intersects() function from the sf package.

st_intersects() will return a list of which points (plaques) are in each polygon (wards). So each row will be a polygon and the ID are the point IDs that intersect the polygon.

In this case we don’t want to use st_join() (which by default uses st_intersects()) as st_join() is like a left join and will retain all the data in on the left. So if a ward is empty, there will still be a row for that ward. So, we use st_intersects() on its own. We could also use other topological（拓扑） relationship functions such as st_within() instead…
```{r}
example<-st_intersects(LondonWardsMerged, BluePlaquesSub)

example
```
Here the polygon with the ID of 6 Kingston upon Thames - Coombe Hill has three plaques within it…we can check this with st_join (or using QGIS by opening the data)…we have three plaques…But note the ID column added is different to the ID of the data…open blueplaques from the environment window and you will see the IDs that were returned in st_intersects().
```{r}
check_example <- LondonWardsMerged%>%
  st_join(BluePlaquesSub)%>%
  filter(ward_name=="Kingston upon Thames - Coombe Hill")
```
Now we just take the length of each list per polygon and add this as new column…
```{r}
library(sf)
points_sf_joined <- LondonWardsMerged%>%
  mutate(n = lengths(st_intersects(., BluePlaquesSub)))%>%
  janitor::clean_names()%>%
  #calculate area
  mutate(area=st_area(.))%>%
  #then density of the points per ward
  mutate(density=n/area)%>%
  #select density and some other variables 
  dplyr::select(density, ward_name, gss_code, n, average_gcse_capped_point_scores_2014)
```
How about a quick choropleth map to see how we are getting on…
```{r}
points_sf_joined<- points_sf_joined %>%                    
  group_by(gss_code) %>%         
  summarise(density = first(density),
          wardname= first(ward_name),
          plaquecount= first(n))

tm_shape(points_sf_joined) +
    tm_polygons("density",
        style="jenks",
        palette="PuOr",
        midpoint=NA,
        popup.vars=c("wardname", "density"),
        title="Blue Plaque Density")
```
So, from the map, it looks as though we might have some clustering of blue plaques in the centre of London so let’s check this with Moran’s I and some other statistics.
# Weight matrix（权重矩阵）
Before being able to calculate Moran’s I and any similar statistics, we need to 
first define a Wij spatial weights matrix
```{r}
library(spdep)
```
First calculate the centroids of all Wards in London
首先计算伦敦所有Wards的质心
```{r}
#First calculate the centroids of all Wards in London

coordsW <- points_sf_joined%>%
  st_centroid()%>%
  st_geometry()
  
# 默认情况下，在使用plot()函数绘制空间对象时可能不显示坐标轴，设置此参数为
# TRUE可以确保坐标轴被绘制出来。
plot(coordsW,axes=TRUE)

```
A spatial weight matrix represents the spatial element of our data

In the first instance we must create a neighbours list — which is a list of all the neighbours. To do so we will use poly2nb() with the argument queen=T saying we want a to use Queens case. Let’s see a summary of the output
```{r}
#create a neighbours list
LWard_nb <- points_sf_joined %>%
  poly2nb(., queen=T)
```
The output is a neighbour list object hence why i’ve called the object LWard_nb. We can get some summary info and plot the matrix.
```{r}
summary(LWard_nb)
```
Here it is telling us that the average number of neighbours is 5.88. Now let’s 
plot the neighbours（邻居图）.
```{r}
#plot them
plot(LWard_nb, st_geometry(coordsW), col="red")
#add a map underneath
plot(points_sf_joined$geometry, add=T)
```
## Matrix style
From the weights list we must now make a spatial weight matrix. This makes a matrix the size of the number of neighbourhoods with values indicating if the elements in the rows are a neighbour or not. The style of weight is especially important here:

B is the basic binary coding (1/0)
W is row standardised (sums over all links to n)
C is globally standardised (sums over all links to n)
U is equal to C divided by the number of neighbours (sums over all links to unity)
S is the variance-stabilizing coding scheme proposed by Tiefelsdorf et al. 1999, p. 167-168 (sums over all links to n).
To understand this further let’s start with binary.
现在我们必须从权重列表中创建一个空间权重矩阵。 这使得矩阵的大小为邻域数量，其值指示行中的元素是否是邻居。 权重的风格在这里尤其重要：

B是基本二进制编码（1/0）
W 是行标准化（对 n 的所有链接求和）
C 是全球标准化的（对 n 的所有链接求和）
U 等于 C 除以邻居数量（所有链接的总和为一）
S 是 Tiefelsdorf 等人提出的方差稳定编码方案。 1999 年，第 17 页。 167-168（n 的所有链接的总和）。
为了进一步理解这一点，让我们从二进制开始。

权重矩阵的风格（我的理解）
```{r}
#create a spatial weights matrix from these weights
Lward.lw <- LWard_nb %>%
  nb2mat(., style="B")

sum(Lward.lw)
```
Summing the binary (1/0) shows that we have 3680 neighbours. Now if we were to do global standardisation this would mean dividing our 625 wards by the total number of neighbours meaning each spatial weight has a value of 0.169.
对二进制求和 (1/0) 显示我们有3680个邻居。现在，如果我们要进行全球标准化，这意味
着将我们的 625 个病房除以邻居总数，这意味着每个空间权重的值为 0.169。

Alternatively we can do row standardisation where 1 is divided by the sum of the number of neighbours in each row. For example, row 1 here sums to 6, meaning each weight would be 0.166 in row 1 only.
或者，我们可以进行行标准化，其中 1 除以每行中邻居数量的总和。 例如，此处第 1 行
的总和为 6，这意味着仅第 1 行中的每个权重为 0.166。
```{r}
sum(Lward.lw[1,])
```
Standardisation permits comparable spatial parameters. But you also need to consider geographical features — should a neighbour be a neighbour if there is a river between them?
标准化允许可比较的空间参数。 但你还需要考虑地理特征——如果邻居之间有一条河，他们还应该是邻居吗？
# Autocorrelation
Now we have defined our Wij matrix, we can calculate the Moran’s I and other associated statistics. However, Moran’s I requires a spatial weight list type object as opposed to matrix, this is simply…
然而，Moran's I 需要一个空间权重列表类型对象而不是矩阵...
```{r}
Lward.lw <- LWard_nb %>%
  nb2listw(., style="C")
```
## Moran’s I
Moran’s I test tells us whether we have clustered values (close to 1) or 
dispersed values (close to -1), we will calculate for the densities rather than 
raw values (think about why this is?).
Moran 的 I 检验告诉我们是否有聚集值（接近1）或分散值（接近-1），我们将计算密度而不是原始值（想想为什么会这样？）。
```{r}
I_LWard_Global_Density <- points_sf_joined %>%
  pull(density) %>%
  as.vector()%>%
  moran.test(., Lward.lw)

I_LWard_Global_Density
```
## Geary’s C
Geary’s C as well..? This tells us whether similar values or dissimilar values 
are clustering
Geary的C也是..？ 这告诉我们相似的值或不相似的值是否聚类
```{r}
C_LWard_Global_Density <- 
  points_sf_joined %>%
  pull(density) %>%
  as.vector()%>%
  geary.test(., Lward.lw)

C_LWard_Global_Density
```
## Getis Ord
Getis Ord General G? This tells us whether high or low values are clustering. If
G > Expected = High values clustering; if G < expected = low values clustering
这告诉我们高值还是低值正在聚类。 如果 G > 预期 = 高值聚类； 
如果 G < 预期 = 低值聚类
```{r}
G_LWard_Global_Density <- 
  points_sf_joined %>%
  pull(density) %>%
  as.vector()%>%
  globalG.test(., Lward.lw)

G_LWard_Global_Density
```
So the global statistics are indicating that we have spatial autocorrelation of 
Blue Plaques in London:
因此，全球统计数据表明伦敦的蓝色斑块具有空间自相关性：
## Summary
The Moran’s I statistic = 0.67 (remember 1 = clustered, 0 = no pattern, 
-1 = dispersed) which shows that we have some distinctive clustering
Moran’s I 统计量 = 0.67（记住 1 = 聚集，0 = 无模式，-1 = 分散），这表明我们有一些独特的聚集

The Geary’s C statistic = 0.41 (remember Geary’s C falls between 0 and 2; 1 means no spatial autocorrelation, <1 - positive spatial autocorrelation or similar values clustering, >1 - negative spatial autocorreation or dissimilar values clustering) which shows that similar values are clustering
Geary 的 C 统计量 = 0.41（请记住 Geary 的 C 介于 0 和 2 之间；1 表示没有空间自相关，<1 - 正空间自相关或相似值聚类，>1 - 负空间自相关或不相似值聚类），这表明相似值是 聚类

The General G statistic = G > expected, so high values are tending to cluster.
一般 G 统计量 = G > 预期，因此高值倾向于聚集。
## Local Moran’s I
We can now also calculate local versions of the Moran’s I statistic (for each 
Ward) and a Getis Ord  Gi* statistic to see where we have hot-spots…

Local Moran’s I is:
The difference between a value and neighbours * the sum of differences between neighbours and the mean
Where the the difference between a value and neighbours is divided by the standard deviation (how much values in neighbourhood vary about the mean)

It returns several columns, of most interest is the Z score. A Z-score is how many standard deviations a value is away (above or below) from the mean. This allows us to state if our value is significantly different than expected value at this location considering the neighours.

We are comparing our value of Moran’s I to that of an expected value (computed from a separate equation that uses the spatial weight matrix, and therefore considers the neighbouring values). We are expecting our value of Moran’s I to be in the middle of the distribution of the expected values. These expected values follow a normal distribution, with the middle part representing complete spatial randomness. This is typically between < -1.65 or > +1.65 standard deviations from the mean

The null hypothesis is always there is complete spatial randomness. A null 
hypothesis means:

no statistical significance exists in a set of given observations

If our value is towards the tails of the distribution then it is unlikely that the value is completely spatially random and we can reject the null hypothesis…as it is not what we expect at this location.

In the example where we use a z-score of >2.58 or <-2.58 we interpret this as…

…> 2.58 or <-2.58 standard deviations away from the mean are significant at the 99% level…this means there is a <1% chance that autocorrelation is not present

the most important parts are knowing:
What we are comparing values to in Local Moran’s I
What the results mean
Why the results could be important
```{r}
#use the localmoran function to generate I for each ward in the city

I_LWard_Local_count <- points_sf_joined %>%
  pull(plaquecount) %>%
  as.vector()%>%
  localmoran(., Lward.lw)%>%
  as_tibble()

I_LWard_Local_Density <- points_sf_joined %>%
  pull(density) %>%
  as.vector()%>%
  localmoran(., Lward.lw)%>%
  as_tibble()

#what does the output (the localMoran object) look like?
slice_head(I_LWard_Local_Density, n=5)
```
There are 5 columns of data. We want to copy some of the columns (the I score 
(column 1) and the z-score standard deviation (column 4)) back into the 
LondonWards spatialPolygonsDataframe
```{r}
points_sf_joined <- points_sf_joined %>%
  mutate(plaque_count_I = as.numeric(I_LWard_Local_count$Ii))%>%
  mutate(plaque_count_Iz =as.numeric(I_LWard_Local_count$Z.Ii))%>%
  mutate(density_I =as.numeric(I_LWard_Local_Density$Ii))%>%
  mutate(density_Iz =as.numeric(I_LWard_Local_Density$Z.Ii))
```
### Mapping
No we can plot a map of the local Moran’s I outputs…

We’ll set the breaks manually based on the rule that data points >2.58 or <-2.58 standard deviations away from the mean are significant at the 99% level (<1% chance that autocorrelation not present); >1.96 - <2.58 or <-1.96 to >-2.58 standard deviations are significant at the 95% level (<5% change that autocorrelation not present). >1.65 = 90% etc.
```{r}
breaks1<-c(-1000,-2.58,-1.96,-1.65,1.65,1.96,2.58,1000)
```
Now create a new diverging colour brewer palette and reverse the order 
using rev() (reverse) so higher values correspond to red
现在创建一个新的发散颜色酿酒调色板并使用 rev() （反向）反转顺序，
因此较高的值对应于红色
```{r}
library(RColorBrewer)
MoranColours<- rev(brewer.pal(8, "RdGy"))
```
Plot on an interactive map
```{r}
tm_shape(points_sf_joined) +
    tm_polygons("plaque_count_Iz",
        style="fixed",
        breaks=breaks1,
        palette=MoranColours,
        midpoint=NA,
        title="Local Moran's I, Blue Plaques in London")
```
This map shows some areas in the centre of London that have relatively high 
scores, indicating areas with lots of blue plaques neighbouring other areas 
with lots of blue plaques.
## Local Getis Ord Gi*
What about the Getis Ord Gi* statistic for hot and cold spots…

This is a very similar concept to Local Moran’s I except it just returns a z-score…remember that a z-score shows how many standard deviations a value (our value) is away (above or below) from the mean (of the expected values)

However, in the case of Getis Ord Gi* this is the local sum (of the 
neighbourhood) compared to the sum of all features
In Moran’s I this is just the value of the spatial unit (e.g. polygon of 
the ward) compared to the neighbouring units.

Here, to be significant (or a hot spot) we will have a high value surrounded by high values. The local sum of these values will be different to the expected sum (think of this as all the values in the area) then where this difference is large we can consider it to be not by chance…

The same z-score criteria then applies as before..

output:
The result of Getis Ord Gi* analysis is an array of Z-scores, one for each pixel [or polygon], which is the number of standard deviations that the pixel [or polygon] and its neighbors are from the global mean. High Z-scores indicate more intense clustering of high pixel values, indicating hot spots. Low Z-scores indicate more intense clustering of low values, indicating cold spots. Individual pixels with high or low values by themselves might be interesting but not necessarily significant.
```{r}
Gi_LWard_Local_Density <- points_sf_joined %>%
  pull(density) %>%
  as.vector()%>%
  localG(., Lward.lw)

head(Gi_LWard_Local_Density)
```
Note that because of the differences in Moran’s I and Getis Ord Gi* there 
will be differences between polyogons that are classed as significant.

Add the Getis Ord Gi* data to the simple feature…
```{r}
points_sf_joined <- points_sf_joined %>%
  mutate(density_G = as.numeric(Gi_LWard_Local_Density))
```
And map the outputs…
```{r}
library(RColorBrewer)

GIColours<- rev(brewer.pal(8, "RdBu"))

#now plot on an interactive map
tm_shape(points_sf_joined) +
    tm_polygons("density_G",
        style="fixed",
        breaks=breaks1,
        palette=GIColours,
        midpoint=NA,
        title="Gi*, Blue Plaques in London")
```
# Other variables
The local Moran’s I and Gi* statistics for wards clearly show that the density 
of blue plaques in the centre of the city exhibits strong (and positive) spatial autocorrelation, but neither of these maps are very interesting. Why not try 
some alternative variables and see what patterns emerge… here I’m going to have 
a look at Average GSCE scores…
```{r}
#use head to see what other variables are in the data file

slice_head(points_sf_joined, n=2)
```
Or print out the class of each column like we did in week 2, although we need to 
drop the geometry.
```{r}
Datatypelist <- LondonWardsMerged %>% 
  st_drop_geometry()%>%
  summarise_all(class) %>%
  pivot_longer(everything(), 
               names_to="All_variables", 
               values_to="Variable_class")

Datatypelist
```
```{r}
I_LWard_Local_GCSE <- LondonWardsMerged %>%
  arrange(GSS_CODE)%>%
  pull(average_gcse_capped_point_scores_2014) %>%
  as.vector()%>%
  localmoran(., Lward.lw)%>%
  as_tibble()

points_sf_joined <- points_sf_joined %>%
  arrange(gss_code)%>%
  mutate(GCSE_LocIz = as.numeric(I_LWard_Local_GCSE$Z.Ii))


tm_shape(points_sf_joined) +
    tm_polygons("GCSE_LocIz",
        style="fixed",
        breaks=breaks1,
        palette=MoranColours,
        midpoint=NA,
        title="Local Moran's I, GCSE Scores")
```
Now the Gi* statistic to look at clusters of high and low scores and explain what the output map is showing and what other questions this can lead us to ask next week...
```{r}
G_LWard_Local_GCSE <- LondonWardsMerged %>%
  dplyr::arrange(GSS_CODE)%>%
  dplyr::pull(average_gcse_capped_point_scores_2014) %>%
  as.vector()%>%
  localG(., Lward.lw)

points_sf_joined <- points_sf_joined %>%
  dplyr::arrange(gss_code)%>%
  dplyr::mutate(GCSE_LocGiz = as.numeric(G_LWard_Local_GCSE))

tm_shape(points_sf_joined) +
    tm_polygons("GCSE_LocGiz",
        style="fixed",
        breaks=breaks1,
        palette=GIColours,
        midpoint=NA,
        title="Gi*, GCSE Scores")
```
