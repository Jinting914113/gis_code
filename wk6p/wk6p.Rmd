---
title: "wk6p"
output: html_document
date: "2023-12-10"
---
# research question
For any given London Borough, are the Blue Plaques within that borough distributed randomly or do they exhibit some kind of dispersed or clustered pattern?
对于任何给定的伦敦行政区，该行政区内的蓝色牌匾是随机分布的还是表现出某种分散或聚集的模式？
## To answer this question, we will make use of some of the Point Pattern Analysis functions found in the spatstat package.
```{r}
#first library a few packages that we will use during the practical
#note you may need to install them first...
library(spatstat)
library(here)
library(sp)
library(tmap)
library(sf)
library(geojson)
library(geojsonio)
library(tmaptools)
```
## Setting up your data
### First, get the London Borough Boundaries
```{r}
LondonBoroughs <- st_read(here::here("statistical-gis-boundaries-london", "ESRI", "London_Borough_Excluding_MHW.shp"))

```
### Pull out London using the str_detect() function from the stringr package in combination with filter() from dplyr (again!). 
We will look for the bit of the district code that relates to London (E09) from the ‘lad15cd’ column data frame of our sf object
```{r}
library(stringr)
BoroughMap <- LondonBoroughs %>%
  dplyr::filter(str_detect(GSS_CODE, "^E09"))%>%
  st_transform(., 27700)

qtm(BoroughMap)
```
### summary(BoroughMap)
```{r}
summary(BoroughMap)
```
### Now get the location of all Blue Plaques in the City and plot it
```{r}
BluePlaques <- st_read("https://s3.eu-west-2.amazonaws.com/openplaques/open-plaques-london-2018-04-08.geojson") %>%
  st_transform(.,27700)
```
```{r}
summary(BluePlaques)
```
```{r}
#plot the blue plaques in the city
tmap_mode("plot")
tm_shape(BoroughMap) +
  tm_polygons(col = NA, alpha = 0.5) +
tm_shape(BluePlaques) +
  tm_dots(col = "blue")
```
### Data cleaning
（1 您可能已经注意到至少有一个蓝色牌匾落在行政区边界之外。错误的斑块会给我们的分析带来问题，因此我们需要将斑块剪到边界）
首先，我们将删除具有相同网格参考的所有斑块，因为这会在稍后的分析中引起问题。
```{r}
#remove duplicates
library(tidyverse)
#这意味着如果有两个或多个蓝色纪念牌具有相同的网格参考，distinct() 函数会删除除了第一个以外的所有纪念牌。
library(sf)
BluePlaques <- distinct(BluePlaques)
```
### Spatial subsetting（空间子集化）
#Now just select the points inside London
Here, the second operator is blank , , - this controls which attributes are 
kept, although I’d rather keep all of them and manipulate with the tidyverse.
```{r}
BluePlaquesSub <- BluePlaques[BoroughMap,]
#check to see that they've been removed
tmap_mode("plot")
tm_shape(BoroughMap) +
  tm_polygons(col = NA, alpha = 0.5) +
tm_shape(BluePlaquesSub) +
  tm_dots(col = "blue")
```
```{r}
# add sparse=false to get the complete matrix.
intersect_indices <-st_intersects(BoroughMap, BluePlaques)
#inside_index <- st_within(BluePlaques, BoroughMap, sparse = FALSE)
```
### Spatial clipping（空间裁剪）
### Spatial joining(we did this in week 5 )
## Study area
```{r}
#extract the borough

# select by attribute
Harrow <- BoroughMap %>%
  filter(., NAME=="Harrow")

#Check to see that the correct borough has been pulled out
tm_shape(Harrow) +
  tm_polygons(col = NA, alpha = 0.5)
```
```{r}
# clip the data to our single borough
BluePlaquesSub <- BluePlaques[Harrow,]
# check that it's worked
tmap_mode("plot")
## tmap mode set to plotting
tm_shape(Harrow) +
  tm_polygons(col = NA, alpha = 0.5) +
tm_shape(BluePlaquesSub) +
  tm_dots(col = "blue")
```
(1) We now have all of our data set up so that we can start the analysis using spatstat. The first thing we need to do is create an observation window for spatstat to carry out its analysis within — we’ll set this to the extent of the Harrow boundary
现在我们已经设置了所有数据，以便我们可以使用 开始分析spatstat。我们需要做的第一件事是创建一个观察窗口，以便spatstat在其中进行分析 - 我们将其设置为哈罗边界的范围
```{r}
# now set a window as the borough boundary
window <- as.owin(Harrow)
plot(window)
```
(2) spatstat has its own set of spatial objects that it works with (one of the delights of R is that different packages are written by different people and many have developed their own data types) — it does not work directly with the SpatialPolygonsDataFrames, SpatialPointsDataFrames or sf objects that we are used to. For point pattern analysis, we need to create a point pattern (ppp) object.
```{r}
#create a sp object
BluePlaquesSub<- BluePlaquesSub %>%
  as(., 'Spatial')
#create a ppp object
BluePlaquesSub.ppp <- ppp(x=BluePlaquesSub@coords[,1],
                          y=BluePlaquesSub@coords[,2],
                          window=window)
```
(# Point pattern analysis)
## Kernel Density Estimation（核密度估计）
(3.1) One way to summarise your point data is to plot the density of your points under a window called a ‘Kernel’. The size and shape of the Kernel affects the density pattern produced, but it is very easy to produce a Kernel Density Estimation (KDE) map from a ppp object using the density() function.
```{r}
BluePlaquesSub.ppp %>%
  density(., sigma=500) %>%
  plot()
```
The sigma value sets the diameter of the Kernel (in the units your map is in — in this case, as we are in British National Grid the units are in metres). Try experimenting with different values of sigma to see how that affects the density estimate.
```{r}
BluePlaquesSub.ppp %>%
  density(., sigma=1000) %>%
  plot()
```
## Quadrat Analysis（样方分析）
So as you saw in the lecture, we are interesting in knowing whether the distribution of points in our study area differs from ‘complete spatial randomness’ — CSR. That’s different from a CRS! Be careful!
正如您在讲座中所看到的，我们有兴趣了解我们研究区域中的点分布是否不同于“完全空间随机性”（CSR）。这与 CRS 不同！当心！
The most basic test of CSR is a quadrat analysis. We can carry out a simple quadrat analysis on our data using the quadrat count function in spatstat. Note, I wouldn’t recommend doing a quadrat analysis in any real piece of analysis you conduct, but it is useful for starting to understand the Poisson distribution…
请注意，我不建议在您进行的任何实际分析中进行样方分析，但它对于开始理解泊松分布很有用
```{r}
#First plot the points
plot(BluePlaquesSub.ppp,
     pch=16,
     cex=0.5, 
     main="Blue Plaques in Harrow")

#now count the points in that fall in a 6 x 6
#grid overlaid across the windowBluePlaquesSub.ppp2<-BluePlaquesSub.ppp %>%
BluePlaquesSub.ppp %>%
  quadratcount(.,nx = 6, ny = 6)%>%
    plot(., add=T, col="red")
```
In our case here, want to know whether or not there is any kind of spatial patterning associated with the Blue Plaques in areas of London. If you recall from the lecture, this means comparing our observed distribution of points with a statistically likely (Complete Spatial Random) distibution, based on the Poisson distribution.
将点的空间分布模式与完全随机空间分布模式做比较
Using the same quadratcount() function again (for the same sized grid) we can save the results into a table:
```{r}
#run the quadrat count
Qcount <- BluePlaquesSub.ppp %>%
  quadratcount(.,nx = 6, ny = 6) %>%
  as.data.frame() %>%
  dplyr::count(Var1=Freq)%>%
  dplyr::rename(Freqquadratcount=n)
```
Check the data type in the first column — if it is factor, we will need to convert it to numeric
```{r}
Qcount %>% 
  summarise_all(class)
```
```{r}
sums <- Qcount %>%
  #calculate the total blue plaques (Var * Freq)
  mutate(total = Var1 * Freqquadratcount) %>%
  dplyr::summarise(across(everything(), sum))%>%
  dplyr::select(-Var1) 

lambda<- Qcount%>%
  #calculate lambda
  mutate(total = Var1 * Freqquadratcount)%>%
  dplyr::summarise(across(everything(), sum)) %>%
  mutate(lambda=total/Freqquadratcount) %>%
  dplyr::select(lambda)%>%
  pull(lambda)
```
使用上面的泊松公式计算预期k是一个正方形中蓝色斑块的数量，
可以在我们表格的第一列中找到......
```{r}
QCountTable <- Qcount %>%
  mutate(Pr=((lambda^Var1)*exp(-lambda))/factorial(Var1))%>%
  #now calculate the expected counts based on our total number of plaques
  #and save them to the table
  mutate(Expected= (round(Pr * sums$Freqquadratcount, 0)))

#Compare the frequency distributions of the observed and expected point patterns
plot(c(1,5),c(0,14), type="n",
xlab="Number of Blue Plaques (Red=Observed,Blue=Expected)", 
     ylab="Frequency of Occurances")
points(QCountTable$Freqquadratcount, 
       col="Red", 
       type="o", 
       lwd=3)
points(QCountTable$Expected, col="Blue", 
       type="o", 
       lwd=3)
```
To check for sure是否随机分布, we can use the quadrat.test() function, built into spatstat. This uses a Chi Squared test to compare the observed and expected frequencies for each quadrant (rather than for quadrant bins, as we have just computed above).
If the p-value of our Chi-Squared test is < 0.05, then we can reject a null hypothesis that says “there is no pattern - i.e. complete spatial randomness - in our data” (think of a null-hypothesis as the opposite of a hypothesis that says our data exhibit a pattern). What we need to look for is a value for p > 0.05. If our p-value is > 0.05 then this indicates that we have CSR and there is no pattern in our points. If it is < 0.05, this indicates that we do have clustering in our points.
```{r}
teststats <- quadrat.test(BluePlaquesSub.ppp, nx = 6, ny = 6)
plot(BluePlaquesSub.ppp,pch=16,cex=0.5, main="Blue Plaques in Harrow")
plot(teststats, add=T, col = "red")
```
## Ripley’s K
One way of getting around the limitations of quadrat analysis is to compare the observed distribution of points with the Poisson random model for a whole range of different distance radii（半径）. This is what Ripley’s K function computes.
计算不同半径范围是Ripley’s K
We can conduct a Ripley’s K test on our data very simply with the spatstat package using the kest() function.
```{r}
K <- BluePlaquesSub.ppp %>%
  Kest(., correction="border") %>%
  plot()
```
当 K 值落在该线上方时，数据似乎在该距离处聚集。当 K 值低于该线时，数据分散。从图中我们可以看到，在距离大约 1300 米之前，蓝色斑块似乎聚集在哈罗，但在大约 1500 米处，分布呈现随机分布，然后在大约 1600 米到 2100 米之间分散。
```{r}
Kval <- as.data.frame(Kest(BluePlaquesSub.ppp, correction = "Ripley"))
```
## Density-based spatial clustering of applications with noise: DBSCAN
直译为“基于密度的带噪声应用空间聚类”
Quadrat 和 Ripley 的 K 分析是有用的探索性技术，可以告诉我们点数据中是否存在空间
聚类，但它们无法告诉我们聚类发生在我们感兴趣的区域中的何处。为了发现这一点，我们
需要使用替代技术——DBSCAN 是一种在空间（物理空间或变量空间）中发现簇的流行技术
```{r}
library(raster)
library(fpc)
```
We will now carry out a DBSCAN analysis of blue plaques in my borough to see if 
there are any clusters present.
我们现在将对我所在行政区的蓝色斑块进行 DBSCAN 分析，以查看是否存在任何集群。
```{r}
#first check the coordinate reference system of the Harrow spatial polygon:
st_geometry(Harrow)
```
DBSCAN requires you to input two parameters: 1. Epsilon - this is the radius within which the algorithm with search for clusters 2. MinPts - this is the minimum number of points that should be considered a cluster
DBSCAN 要求您输入两个参数： 1. Epsilon - 这是搜索聚类的算法的半径 2. MinPts - 这是应被视为聚类的最小点数
Based on the results of the Ripley’s K analysis earlier, we can see that we are getting clustering up to a radius of around 1200m, with the largest bulge in the graph at around 700m. Therefore, 700m is probably a good place to start and we will begin by searching for clusters of at least 4 points…
根据之前 Ripley's K 分析的结果，我们可以看到我们的聚类半径约为 1200m，图中最大的凸起位于 700m 左右。因此，700m 可能是一个不错的起点，我们将从搜索至少 4 个点的簇开始......
```{r}


#first extract the points from the spatial points data frame
library(sf)

BluePlaquesSubPoints <- BluePlaquesSubdu %>%
  st_coordinates() %>%
  as.data.frame()



#now run the dbscan analysis
db <- BluePlaquesSubPoints %>%
  fpc::dbscan(.,eps = 700, MinPts = 4)

#now plot the results
plot(db, BluePlaquesSubPoints, main = "DBSCAN Output", frame = F)
plot(Harrow$geometry, add=T)
```
also use kNNdistplot() from the dbscan pacakge to find a suitable eps value based on the ‘knee’ in the plot…
```{r}
# used to find suitable eps value based on the knee in plot
# k is no of nearest neighbours used, use min points
library(dbscan)

BluePlaquesSubPoints%>%
  dbscan::kNNdistplot(.,k=4)
```
This plot shows for each point the average distance to the k neighbours, which 
are then plotted in ascending order. The knee is where this value (of distance 
to neighbours) increases.
produce a much nicer plot by extracting the useful information from the DBSCAN 
output and use ggplot2 to produce a much cooler map…
```{r}
library(ggplot2)
db
db$cluster
# We can now add this cluster membership info back into our dataframe
BluePlaquesSubPoints<- BluePlaquesSubPoints %>%
  mutate(dbcluster=db$cluster)
# 
#


```








